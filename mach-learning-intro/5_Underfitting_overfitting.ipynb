{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eab2d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "melb_file_path = 'assets/input/melb_data.csv'\n",
    "melbourne_data = pd.read_csv(melb_file_path)\n",
    "prep_melb_data = melbourne_data.dropna(axis=0)\n",
    "\n",
    "y = prep_melb_data.Price\n",
    "features = ['Rooms', 'Bathroom', 'Landsize', 'Lattitude', 'Longtitude']\n",
    "X = prep_melb_data[features]\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2057e73",
   "metadata": {},
   "source": [
    "# Overfitting y Underfitting\n",
    "Ahora que tenemos una herramienta para medir la precisión de nuestro modelo, podríamos utilizarla para evaluar la calidad de varios modelos y luego quedarnos con el que mejor performance muestre.\n",
    "\n",
    "Recordamos, también, que **una de las características de un modelo es su profundidad** (cantidad de *splits*, divisiones en grupos). Cada nivel que agregamos, nos divide el grupo inicial en 2, por tanto, si, por ejemplo, tuviésemos 10 niveles, esto nos daría un total de 1024 grupos de elementos, donde **cada grupo, a medida que crece la profundidad, contendrá una menor cantidad de elementos**. Esto generará que nuestro modelo se ajuste con gran precisión a cada uno de esos elementos (de entrenamiento) pero como contrapartida, **si se ajusta tanto a los elementos particulares perderá precisión al presentarse nuevos elementos**. Esto es lo que se conoce como **overfitting**.\n",
    "\n",
    "Por otro lado, si utilizáramos un **bajo nivel de profundidad, diviendo nuestros datos en 2 o 4 grupos** por ejemplo, nuestro modelo **no lograría capturar distinciones o patrones importantes**, además que cada grupo contendría una dispersión importante entre sus valores. Esto es lo que se conoce como **underffiting**\n",
    "\n",
    "\n",
    "*foto no esta cargando*\n",
    "\n",
    "Pasando a la práctica, una forma de controlar la profundidad de nuestro modelo, es mediante *max_leaf_nodes*. Podremos, entonces, medir la precisión de diferentes modelos según su profundidad y luego quedarnos con el que presente un mejor resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bf705b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo de mejor MAE fue el que tenía 500 leaves, con un error de: 260,235.05\n",
      "Max leaves: 5    \t\t Mean Error: 385696.54278937966\n",
      "Max leaves: 50    \t\t Mean Error: 279794.61143891385\n",
      "Max leaves: 500    \t\t Mean Error: 260235.05178963946\n",
      "Max leaves: 5000    \t\t Mean Error: 273765.3918657198\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def get_mae(max_leaves, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaves, random_state=1)\n",
    "    model.fit(train_X, train_y)\n",
    "    predictions = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, predictions)\n",
    "    return(mae)\n",
    "\n",
    "candidate_leaves = [5, 50, 500, 5000]\n",
    "maes = {leaves: get_mae(leaves,train_X, val_X, train_y, val_y) for leaves in candidate_leaves}\n",
    "\n",
    "# Modelo con mejor performance (menor MAE)\n",
    "# esta funcion para encontrar el minimo la saque de internet\n",
    "best_tree_size = min(maes, key=maes.get)\n",
    "print('El modelo de mejor MAE fue el que tenía {} leaves, con un error de: {:,.2f}'.format(best_tree_size, maes[best_tree_size]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3443a3a2",
   "metadata": {},
   "source": [
    "**IMPORTANTE**: luego de encontrar el **mejor nivel de profundidad** para nuestro modelo, se **construye el modelo final, incluyendo tanto la data de entrenamiento como la de validación**, ya que esto nos permitirá obtener la **mayor precisión** posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b07a5e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_leaf_nodes=500, random_state=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=1)\n",
    "final_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24967ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
